{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# load the training data\n",
    "train_data = json.load(open(\"genre_train.json\", \"r\"))\n",
    "X = train_data['X']\n",
    "Y = train_data['Y'] # id mapping: 0 - Horror, 1 - Science Fiction, 2 - Humor, 3 - Crime Fiction\n",
    "docid = train_data['docid'] # these are the ids of the books which each training example came from\n",
    "\n",
    "# load the test data\n",
    "# the test data does not have labels, our model needs to generate these\n",
    "test_data = json.load(open(\"genre_test.json\", \"r\"))\n",
    "Xt = test_data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional,Dense, Flatten, Conv1D,MaxPooling1D,Dropout,BatchNormalization,GlobalAveragePooling1D,Embedding\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras import utils as np_utils\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {'X':X,\n",
    "       'Y':Y}\n",
    "data = pd.DataFrame(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {'Xt':Xt}\n",
    "data_tests = pd.DataFrame(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentence):\n",
    "    stop = '[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+'\n",
    "    sentence = re.sub(stop, '', sentence)\n",
    "    return sentence.split()\n",
    "\n",
    "sentences  = data.X.apply(split_sentence)\n",
    "sentences_Xt  = data_tests.Xt.apply(split_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_document(doc):\n",
    "#     processed_doc = []\n",
    "#     for i in range(len(doc)):\n",
    "#         words_in_doc = doc[i]\n",
    "#         list_term = [t.lower() for t in words_in_doc]\n",
    "#         list_term = [stem(t)for t in list_term if t not in stopwords]\n",
    "#         processed_doc.append(list_term)\n",
    "#     return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = process_document(sentences)\n",
    "# sentences_Xt = process_document(sentences_Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "# from nltk.stem import PorterStemmer\n",
    "# stem = nltk.PorterStemmer().stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 训练Word2Vec\n",
    "\"\"\"\n",
    "# 嵌入的维度\n",
    "embedding_vector_size = 200\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,vector_size=embedding_vector_size,\n",
    "    min_count=1, window=3, workers=4)\n",
    "\n",
    "# 取得所有单词  \n",
    "vocab_list = list(w2v_model.wv.index_to_key)\n",
    "# 每个词语对应的索引\n",
    "word_index = {word: index for index, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列化\n",
    "def get_index(sentence):\n",
    "    global word_index\n",
    "    sequence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sequence.append(word_index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sequence\n",
    "X_data = list(map(get_index, sentences))\n",
    "Xt_data = list(map(get_index, sentences_Xt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截长补短\n",
    "maxlen = 300\n",
    "X_pad = pad_sequences(X_data, maxlen=maxlen)\n",
    "Xt_pad = pad_sequences(Xt_data, maxlen=maxlen)\n",
    "# 取得标签\n",
    "Y_value = data.Y.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 构建分类模型\n",
    "\"\"\"\n",
    "# 让 Keras 的 Embedding 层使用训练好的Word2Vec权重\n",
    "embedding_matrix = w2v_model.wv.vectors\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(\n",
    "#     input_dim=embedding_matrix.shape[0],\n",
    "#     output_dim=embedding_matrix.shape[1],\n",
    "#     input_length=maxlen,\n",
    "#     weights=[embedding_matrix],\n",
    "#     trainable=False))\n",
    "# model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(128, activation='sigmoid'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# model.add(Embedding(\n",
    "#     input_dim=embedding_matrix.shape[0],\n",
    "#     output_dim=embedding_matrix.shape[1],\n",
    "#     input_length=maxlen,\n",
    "#     weights=[embedding_matrix],\n",
    "#     trainable=False))\n",
    "# model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1,return_sequences=True)))\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(\n",
    "#     input_dim=embedding_matrix.shape[0],\n",
    "#     output_dim=embedding_matrix.shape[1],\n",
    "#     input_length=maxlen,\n",
    "#     weights=[embedding_matrix],\n",
    "#     trainable=False))\n",
    "# #model.add(Embedding(max_features,128,input_length=maxlen))\n",
    "# model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "# #输出64维向量\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# #输出64维向量\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4,activation='softmax')) #二分类模型一般采用sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    input_length=maxlen,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = keras.utils.np_utils.to_categorical(Y_value, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 3.7354 - accuracy: 0.5870 - val_loss: 2.7893 - val_accuracy: 0.4104\n",
      "Epoch 2/6\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 1.6266 - accuracy: 0.6752 - val_loss: 2.9824 - val_accuracy: 0.3156\n",
      "Epoch 3/6\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 1.2427 - accuracy: 0.7197 - val_loss: 1.9942 - val_accuracy: 0.5837\n",
      "Epoch 4/6\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 1.0182 - accuracy: 0.7716 - val_loss: 5.3937 - val_accuracy: 0.7156\n",
      "Epoch 5/6\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 1.0837 - accuracy: 0.7741 - val_loss: 2.7378 - val_accuracy: 0.4193\n",
      "Epoch 6/6\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6388 - accuracy: 0.8289 - val_loss: 2.3324 - val_accuracy: 0.5615\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_pad,\n",
    "    y=one_hot_labels,\n",
    "    batch_size=32,\n",
    "    epochs=6,\n",
    " validation_split=.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=model.predict(Xt_pad) \n",
    "result  =np.argmax(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open(\"out.csv\", \"w\")\n",
    "fout.write(\"Id,Y\\n\")\n",
    "for i, line in enumerate(result): # Y_test_pred is in the same order as the test data\n",
    "    fout.write(\"%d,%d\\n\" % (i, line))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31606\n",
       "2    11525\n",
       "0     3109\n",
       "3      943\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('out.csv')\n",
    "df['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sample: 6743, #feature: 38896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df=1, stop_words=\"english\", decode_error='ignore')\n",
    "X_train_counts = count_vect.fit_transform(texts['X'])\n",
    "num_samples, num_features = X_train_counts.shape\n",
    "print(\"#sample: %d, #feature: %d\" % (num_samples, num_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sample: 6743, #feature: 38896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "num_samples, num_features = X_train_tfidf.shape\n",
    "print(\"#sample: %d, #feature: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "\n",
    "clf = clf.fit(X_train_tfidf, texts['Y'])\n",
    "\n",
    "# 对新的样本进行预测\n",
    "docs_new = Xt\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "pred = clf.predict(X_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in range(len(pred)) if pred[i]==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in range(0,len(balanced_Y)) if balanced_Y[i]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow.python.keras.engine\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "        #inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "\n",
    "\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "\n",
    "        QK = QK / (64**0.5)\n",
    "\n",
    "        QK = K.softmax(QK)\n",
    "\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "\n",
    "        V = K.batch_dot(QK,WV)\n",
    "\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ.shape (None, 300, 300)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 300, 300)\n",
      "QK.shape (None, 300, 300)\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_42 (Embedding)     (None, 300, 300)          15863400  \n",
      "_________________________________________________________________\n",
      "self__attention_23 (Self_Att (None, 300, 300)          270000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_19  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 1204      \n",
      "=================================================================\n",
      "Total params: 16,134,604\n",
      "Trainable params: 16,134,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "        #inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "\n",
    "\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "\n",
    "        QK = QK / (300**0.5)\n",
    "\n",
    "        QK = K.softmax(QK)\n",
    "\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "\n",
    "        V = K.batch_dot(QK,WV)\n",
    "\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('Loading data...')\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# #标签转换为独热码\n",
    "# y_train, y_test = pd.get_dummies(y_train),pd.get_dummies(y_test)\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "\n",
    "#%%数据归一化处理\n",
    "\n",
    "# maxlen = 300\n",
    "\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# print('x_train shape:', x_train.shape)\n",
    "\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "#%%\n",
    "\n",
    "batch_size = 32\n",
    "from keras.models import Model\n",
    "#from keras.optimizers import SGD,Adam\n",
    "from keras.layers import *\n",
    "#from Attention_keras import Attention,Position_Embedding\n",
    "\n",
    "\n",
    "S_inputs = Input(shape=(300,), dtype='int32')\n",
    "\n",
    "embeddings = Embedding(len(vocab_list), 300)(S_inputs)\n",
    "\n",
    "\n",
    "O_seq = Self_Attention(300)(embeddings)\n",
    "\n",
    "\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "\n",
    "outputs = Dense(4, activation='softmax')(O_seq)\n",
    "\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "# try using different optimizers and different optimizer configs\n",
    "opt = Adam(lr=0.0002,decay=0.00001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(loss=loss,\n",
    "\n",
    "             optimizer='adam',\n",
    "\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/2\n",
      "WQ.shape (None, 300, 300)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 300, 300)\n",
      "QK.shape (None, 300, 300)\n",
      "WQ.shape (None, 300, 300)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 300, 300)\n",
      "QK.shape (None, 300, 300)\n",
      "211/211 [==============================] - 108s 507ms/step - loss: 0.7737 - accuracy: 0.7234\n",
      "Epoch 2/2\n",
      "211/211 [==============================] - 110s 519ms/step - loss: 0.3309 - accuracy: 0.8821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8c8f1a50>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_pad, one_hot_labels,\n",
    "\n",
    "         batch_size=batch_size,\n",
    "\n",
    "         epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ.shape (None, 300, 300)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 300, 300)\n",
      "QK.shape (None, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "predicted=model.predict(Xt_pad) \n",
    "result  =np.argmax(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open(\"out.csv\", \"w\")\n",
    "fout.write(\"Id,Y\\n\")\n",
    "for i, line in enumerate(result): # Y_test_pred is in the same order as the test data\n",
    "    fout.write(\"%d,%d\\n\" % (i, line))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    40724\n",
       "2     5380\n",
       "3      901\n",
       "0      178\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('out.csv')\n",
    "df['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "62\n",
    "1    38809\n",
    "2     6604\n",
    "3     1020\n",
    "0      750\n",
    "\n",
    "\n",
    "61\n",
    "1    39889\n",
    "2     5024\n",
    "0     1145\n",
    "3     1125\n",
    "\n",
    "\n",
    "57\n",
    "\n",
    "1    40480\n",
    "2     5759\n",
    "3      829\n",
    "0      115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47183"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Attention_keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-2be58a6bc9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mAttention_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPosition_Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Attention_keras'"
     ]
    }
   ],
   "source": [
    "from Attention_keras import Attention,Position_Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.86"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)*0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47183"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
